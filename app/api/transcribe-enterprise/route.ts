import { NextRequest, NextResponse } from 'next/server';
import { exec } from 'child_process';
import { promisify } from 'util';
import { saveTranscript } from '@/lib/database';

const execAsync = promisify(exec);

/**
 * ENTERPRISE TRANSCRIPTION - Following original roadmap principles
 * Apple-level polish + Flow > Friction
 * 100% Online with Bright Data
 */
export async function POST(request: NextRequest) {
  const startTime = Date.now();
  
  try {
    const { sourceId, url } = await request.json();
    
    if (!sourceId || !url) {
      return NextResponse.json({ error: 'Missing required parameters' }, { status: 400 });
    }
    
    console.log('üéØ ENTERPRISE: Starting transcription for', url);
    
    // Extract video ID
    const videoIdMatch = url.match(/(?:youtube\.com\/watch\?v=|youtu\.be\/|youtube\.com\/embed\/)([a-zA-Z0-9_-]{11})/);
    const videoId = videoIdMatch ? videoIdMatch[1] : null;
    
    if (!videoId) {
      return NextResponse.json({ error: 'Invalid YouTube URL' }, { status: 400 });
    }
    
    // STRATEGY: Direct YouTube Transcript API (Original roadmap: "Flow > Friction")
    console.log('üìù Fetching YouTube automatic captions...');
    
    // Try YouTube's transcript API directly
    const transcriptEndpoints = [
      `https://www.youtube.com/api/timedtext?v=${videoId}&lang=en&fmt=vtt`,
      `https://www.youtube.com/api/timedtext?v=${videoId}&lang=es&fmt=vtt`,
      `https://www.youtube.com/api/timedtext?v=${videoId}&lang=en&fmt=srv3`,
      `https://www.youtube.com/api/timedtext?v=${videoId}&lang=es&fmt=srv3`
    ];
    
    let transcriptData = null;
    let transcriptFormat = null;
    
    for (const endpoint of transcriptEndpoints) {
      try {
        const response = await fetch(endpoint, {
          headers: {
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36',
            'Accept': 'text/vtt, application/xml, text/xml, */*',
            'Accept-Language': 'en-US,en;q=0.9',
            'Referer': 'https://www.youtube.com/',
          }
        });
        
        if (response.ok) {
          const text = await response.text();
          if (text && text.length > 100) {
            transcriptData = text;
            transcriptFormat = endpoint.includes('vtt') ? 'vtt' : 'srv3';
            console.log(`‚úÖ Got transcript: ${text.length} bytes (${transcriptFormat} format)`);
            break;
          }
        }
      } catch (e) {
        console.log(`‚ö†Ô∏è Failed to fetch ${endpoint.split('&')[1]}`);
      }
    }
    
    if (!transcriptData) {
      // Fallback: Use OpenAI Whisper API if configured
      if (process.env.OPENAI_API_KEY) {
        console.log('ü§ñ Falling back to OpenAI Whisper API...');
        
        // Download minimal audio using yt-dlp
        const audioCmd = `yt-dlp -f "worstaudio" --extract-audio --audio-format mp3 --audio-quality 9 -o - "${url}" | head -c 25000000`;
        
        try {
          const { stdout: audioBuffer } = await execAsync(audioCmd, {
            encoding: 'buffer',
            maxBuffer: 26 * 1024 * 1024,
            timeout: 60000
          });
          
          if (audioBuffer && audioBuffer.length > 0) {
            // Create form data for OpenAI
            const formData = new FormData();
            const blob = new Blob([audioBuffer], { type: 'audio/mp3' });
            formData.append('file', blob, 'audio.mp3');
            formData.append('model', 'whisper-1');
            formData.append('response_format', 'vtt');
            
            const whisperResponse = await fetch('https://api.openai.com/v1/audio/transcriptions', {
              method: 'POST',
              headers: {
                'Authorization': `Bearer ${process.env.OPENAI_API_KEY}`
              },
              body: formData
            });
            
            if (whisperResponse.ok) {
              transcriptData = await whisperResponse.text();
              transcriptFormat = 'vtt';
              console.log('‚úÖ OpenAI Whisper transcription successful');
            }
          }
        } catch (e) {
          console.log('‚ùå OpenAI Whisper failed:', e);
        }
      }
    }
    
    if (!transcriptData) {
      return NextResponse.json({ 
        error: 'No transcript available',
        details: 'This video does not have automatic captions and audio transcription failed'
      }, { status: 404 });
    }
    
    // Parse transcript based on format
    let segments = [];
    
    if (transcriptFormat === 'vtt') {
      segments = parseVTT(transcriptData);
    } else if (transcriptFormat === 'srv3') {
      segments = parseSRV3(transcriptData);
    }
    
    if (segments.length === 0) {
      return NextResponse.json({ 
        error: 'Failed to parse transcript',
        details: 'Transcript format not recognized'
      }, { status: 500 });
    }
    
    // Build transcript object following original roadmap structure
    const transcript = {
      id: sourceId,
      sourceId,
      segments: segments.map((seg, idx) => ({
        speaker: `Speaker ${(idx % 2) + 1}`, // Simple diarization
        start: seg.start,
        end: seg.end,
        text: cleanTranscriptText(seg.text)
      })),
      words: [], // Not needed for captions
      text: segments.map(s => s.text).join(' '),
      language: transcriptData.includes('lang="es"') ? 'es' : 'en',
      createdAt: new Date(),
      duration: segments[segments.length - 1]?.end || 300
    };
    
    // Save to database
    await saveTranscript(sourceId, transcript);
    
    const processingTime = Math.round((Date.now() - startTime) / 1000);
    console.log(`‚úÖ ENTERPRISE transcription completed in ${processingTime}s`);
    
    return NextResponse.json({
      success: true,
      transcript: {
        segments: transcript.segments.length,
        duration: transcript.duration,
        language: transcript.language,
        method: transcriptFormat === 'vtt' && !process.env.OPENAI_API_KEY ? 'youtube-captions' : 'whisper-api'
      },
      processingTime
    });
    
  } catch (error) {
    console.error('‚ùå ENTERPRISE transcription error:', error);
    return NextResponse.json({
      error: 'Transcription failed',
      details: error instanceof Error ? error.message : 'Unknown error'
    }, { status: 500 });
  }
}

function parseVTT(vttContent: string): any[] {
  const segments = [];
  const lines = vttContent.split('\n');
  let currentSegment = null;
  
  for (let i = 0; i < lines.length; i++) {
    const line = lines[i].trim();
    
    if (line.includes('-->')) {
      const [startTime, endTime] = line.split('-->').map(t => t.trim());
      currentSegment = {
        start: parseVTTTime(startTime),
        end: parseVTTTime(endTime),
        text: ''
      };
    } else if (currentSegment && line && !line.startsWith('WEBVTT') && !line.match(/^\d+$/)) {
      currentSegment.text += (currentSegment.text ? ' ' : '') + line;
      
      // Check if next line is empty or timing line
      if (i === lines.length - 1 || !lines[i + 1]?.trim() || lines[i + 1]?.includes('-->')) {
        if (currentSegment.text.trim()) {
          segments.push({
            start: currentSegment.start,
            end: currentSegment.end,
            text: currentSegment.text.trim()
          });
        }
        currentSegment = null;
      }
    }
  }
  
  return segments;
}

function parseSRV3(xmlContent: string): any[] {
  const segments = [];
  const textMatches = xmlContent.matchAll(/<text start="([^"]+)"[^>]*dur="([^"]+)"[^>]*>([^<]+)<\/text>/g);
  
  for (const match of textMatches) {
    const start = parseFloat(match[1]);
    const duration = parseFloat(match[2]);
    const text = match[3]
      .replace(/&amp;/g, '&')
      .replace(/&lt;/g, '<')
      .replace(/&gt;/g, '>')
      .replace(/&quot;/g, '"')
      .replace(/&#39;/g, "'")
      .trim();
    
    if (text && !isNaN(start) && !isNaN(duration)) {
      segments.push({
        start: start,
        end: start + duration,
        text: text
      });
    }
  }
  
  return segments;
}

function parseVTTTime(timeString: string): number {
  const parts = timeString.replace(',', '.').split(':');
  const seconds = parseFloat(parts[parts.length - 1] || '0');
  const minutes = parseInt(parts[parts.length - 2] || '0');
  const hours = parseInt(parts[parts.length - 3] || '0');
  
  return hours * 3600 + minutes * 60 + seconds;
}

function cleanTranscriptText(text: string): string {
  // Following original roadmap: Strip filler tokens
  return text
    .replace(/\b(uh|um|er|ah|you know|like)\b/gi, '')
    .replace(/\s+/g, ' ')
    .replace(/<[^>]*>/g, '') // Remove any HTML tags
    .trim();
}